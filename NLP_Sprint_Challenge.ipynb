{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Sprint Challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sulemc/nlp-sprint-challenge/blob/master/NLP_Sprint_Challenge.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "B513-nWV9iSm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sprint Challenge: Natural Language Processing"
      ]
    },
    {
      "metadata": {
        "id": "Xo5zIUFW9tLl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this Sprint Challenge, you will get an opportunity to work on additional exercises that will help further crystalize the concepts that you have been exposed to this week.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "C2epBzB0J8RT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question 1**: Load the dataset  (only the first 1000 rows)related to restaurant reviews (Dataset: https://www.dropbox.com/s/i4zh5fb82x7i3sm/restaurant-test.csv?raw=1). \n",
        "\n",
        "This data set is a slight variation of the data set that you worked on in the project assignment.\n",
        "\n",
        "Pre-process the dataset:\n",
        "\n",
        "a) You will need to eliminate punctuations\n",
        "\n",
        "b) You will have to deal with/remove stopwords\n",
        "\n",
        "c) Tokenize the text\n",
        "\n",
        "d) Stem or Lemmatize to determine the base form of the words"
      ]
    },
    {
      "metadata": {
        "id": "RjSZ_NFcJ7Rv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v0JTuUR4K1Ct",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question 2**: **Perform Vectorization** - you will apply 3 different vectorization techniques. Each technique will generate similar document term matrices where the rows of the matrix will represent the respective text messages and the columns will represent each word or a combination of words. Note that the biggest difference between the techniques is the value depicted in the actual cells of the matrix.\n",
        "\n",
        "1) Create a document term matrix based on the count of the words in the document. You may want to restrict the # of features/columns based on the top most features ordered by term frequency across the document\n",
        "\n",
        "2) Create a bigram vector using a combination of adjacent words. In this case, n=2\n",
        "\n",
        "3) Create a TF-IDF vector wherein the cells of the matrix contain values (i.e. weights) to depict how important a word is to an individual review"
      ]
    },
    {
      "metadata": {
        "id": "vEC8GiPuLMUc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question 3: ** \n",
        "\n",
        "**a)** Train the Word2vec model with tokenized content; size of the word vectors is 5; the word should show-up at least once in the raw content\n",
        "\n",
        "**b)** List the number of words in the model's vocabulary\n",
        "\n",
        "**c)**Examine word similarity to the word \"awesome\" and \"loves\"\n",
        "\n",
        "**d)**Consider each review to be a document on its own. Examine document similarity with Doc2vec to any body of text of your choice"
      ]
    },
    {
      "metadata": {
        "id": "a1vpsafyK_F9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zK36_yFRNiKL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question 4: **Iterate over the reviews and output the polarity and subjectivity of the respective tweets. What is the underlying trend with respect to polarity (i.e. positive or negative)?"
      ]
    },
    {
      "metadata": {
        "id": "CRUI4bmaO2tf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question 5:** Train a Naive Bayes classifier on a subset of the movie_reviews data set which is part of the NLTK corpus. Once the classifier has been trained, evaluate it's accuracy by testing it against a subset of data from the movie_reviews data set. \n",
        "\n",
        "**Step 1**: Import the data set from the nltk corpus\n",
        "\n",
        "**Step 2**: Examine the categories within the movie_reviews data set\n",
        "\n",
        "**Step 3**: Examine the files that constitute the movie_reviews data set\n",
        "\n",
        "**Step 4**: Store a list of words for each file ID, followed by the positive or negative label in one big list.\n",
        "*Note *that each review has its own ID\n",
        "\n",
        "**Step 5**:  Randomize the items of a list in place. This is required since there is a high likelihood that we would train on all of the negatives, some positives, and then test only against positives\n",
        "\n",
        "**Step 6**: Find the most-used words in the text and count how often they are used\n",
        "\n",
        "**Step 7**: Select the top 5,000 most common words\n",
        "\n",
        "**Step 8**: Iterate  over the top 5,000  words and build a **feature set **that contains words from across the reviews including whether the word is among the top 5000 words and the corresponding category \n",
        "\n",
        "**Step 9**: First split the feature set list in a training and testing subsets\n",
        "\n",
        "\n",
        "**Step 10**: Train the Naive Bayes Classifier model with the training data set\n",
        "\n",
        "**Step 11**: Evaluate the accuracy of the model against the testing subset\n",
        "\n",
        "**Step 12**: Output the most informative features - (for example: which features appear more often in a postive review as compared to a negative review or vice versa).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jepL1FCNPQPT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eEcYiMA-PTac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question 6: ** Write a blog post on how you could apply one or more of the techniques that you have learned this week. The recommended approach would be explore a data set (Netflix reviews or Yelp reviews) and utilize text processing that we have covered to derive insights from the body of text.\n"
      ]
    }
  ]
}